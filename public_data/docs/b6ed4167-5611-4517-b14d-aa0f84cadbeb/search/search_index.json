{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to PromptGuard Docs Don't know where to go? Just start with Lab High Level Design . Site layout Lab Environment Introduction High Level Design Transaction Risk Score Model Evaluation Pipeline Model Traning Pipeline Common Components Redis Kafka SeaweedFS Postgres Apache Airflow Prod Environment API Standard","title":"Home"},{"location":"#welcome-to-promptguard-docs","text":"Don't know where to go? Just start with Lab High Level Design .","title":"Welcome to PromptGuard Docs"},{"location":"#site-layout","text":"Lab Environment Introduction High Level Design Transaction Risk Score Model Evaluation Pipeline Model Traning Pipeline Common Components Redis Kafka SeaweedFS Postgres Apache Airflow Prod Environment API Standard","title":"Site layout"},{"location":"api-std/","text":"API services standard Timeout and Retry Mechanism Timeout Policy An API request is considered timed out if no response is received within 60 seconds. Retry Mechanism If a timeout occurs, the system will automatically retry the request up to 3 times. Each retry attempt will be made after a fixed delay of 3 seconds, measured from the previous attempt (not cumulative or exponential). If all three retry attempts fail, the request is considered failed, and appropriate error handling will be triggered. Action Scenario Timeout waiting Delay before attemp Actual call Immidiate call API service 60 sec + Delay 3 second 1st retry 1st retry call service 60 sec + Delay 3 second 2nd retry 2nd retry call service 60 sec + Delay 3 second 3rd retry 3rd retry call service 60 sec - Error monitor trigger 3rd retry failed, error handling will be triggered. - -","title":"API Standard"},{"location":"api-std/#api-services-standard","text":"","title":"API services standard"},{"location":"api-std/#timeout-and-retry-mechanism","text":"","title":"Timeout and Retry Mechanism"},{"location":"api-std/#timeout-policy","text":"An API request is considered timed out if no response is received within 60 seconds.","title":"Timeout Policy"},{"location":"api-std/#retry-mechanism","text":"If a timeout occurs, the system will automatically retry the request up to 3 times. Each retry attempt will be made after a fixed delay of 3 seconds, measured from the previous attempt (not cumulative or exponential). If all three retry attempts fail, the request is considered failed, and appropriate error handling will be triggered. Action Scenario Timeout waiting Delay before attemp Actual call Immidiate call API service 60 sec + Delay 3 second 1st retry 1st retry call service 60 sec + Delay 3 second 2nd retry 2nd retry call service 60 sec + Delay 3 second 3rd retry 3rd retry call service 60 sec - Error monitor trigger 3rd retry failed, error handling will be triggered. - -","title":"Retry Mechanism"},{"location":"key-matrics/","text":"Key design metrics We compose the key design metrics here for ease of reference. TODO... - Kafka sizing - Redis sizing - Each components sizing TODO... Make decision if we should put them separately","title":"Key design metrics"},{"location":"key-matrics/#key-design-metrics","text":"We compose the key design metrics here for ease of reference. TODO... - Kafka sizing - Redis sizing - Each components sizing TODO... Make decision if we should put them separately","title":"Key design metrics"},{"location":"lab-env/data-prep/","text":"Real-time Transaction Data Preparation Real-time transaction data prep consumes message from the Kafka topic: promptguard.transactionscore.from-pp . There are 3 sub modules here: Transaction recorder: to save transactions for using in daily feature preparation and evaluation pipeline (data profile shifing check) Proxy Updater: to update bank account proxy id Prediction feature preparation: to use in prediction pipeline graph TD LS[PP Log Streamer] -- produce --> KK[Kafka <br>topic: promptguard.transactionscore.from-pp] KK -- \"consume msg\" --> TR[\"Transaction Recorder<br> (Lookup & Credit transfer)<br> (Go)\"] TR -- accumulate & save --> SF[\"SeaweedFS <br> (Parquet)\"] KK -- \"consume msg\" --> PU[\"Proxy Updater<br> (only Lookup msg)<br> (Go)\"] PU -- \"Update proxy and reverse proxy\" --> R[\"Redis\"] KK -- \"consume msg\" --> FT[\"Prediction Feature Updater<br> (only Credit transfer msg)<br> (Go)\"] FT -- \"Update account recent transactions (10-min feature)\" --> R Sequence Diagram sequenceDiagram participant LE as Log extractor (Blend / Loki) participant KK as Kafka Topic participant TA as Transaction recorder participant RP as Real-time feature prep participant R as Redis participant SF as SeaweedFS participant DDP as Daily feature prep participant PUD as Proxy updater LE->>KK: publish anonym txn note over KK: promptguard.transactionscore.from-pp TA->>KK: consume txn data activate TA activate KK KK-->>TA: txn data deactivate KK TA->>TA: batch packing TA->>SF: batch data store deactivate TA PUD->>KK: consume txn data activate PUD activate KK KK-->>PUD: txn data deactivate KK PUD->>R: update proxy to Redis deactivate PUD RP->>KK: consume txn data activate RP activate KK KK-->>RP: txn data deactivate KK RP->>R: update recent txn feature deactivate RP DDP->>SF: daily read txn data activate DDP activate SF SF-->>DDP: data deactivate SF DDP->>DDP: cal daily features DDP->>R: save daily features DDP->>SF: save daily features deactivate DDP PromptPay Transaction recorder This module will consume the PromptPay transaction from promptguard.transactionscore.from-pp topic. It accommulates the transaction data then saves to SeaweedFS, in Parquet (Zstd) format. graph LR LS[PP Log Streamer] -- produce --> KK[Kafka <br>topic: promptguard.transactionscore.from-pp] KK -- \"consume msg\" --> TR[\"Transaction Recorder<br> (Lookup & Credit transfer)<br> (Go)\"] TR -- accumulate & save --> SF[\"SeaweedFS <br> (Parquet)\"] The transaction schema is defined here: PromptPay message schema . These saved transactions will be used by Daily Feature Preparation , Evaluation , and Training pipelines. Proxy Updater This module consumes the PromptPay transaction from promptguard.transactionscore.from-pp topic. It extracts proxy and actual account information, if any, then update to proxy mapper in Redis. graph LR KK[Kafka <br>topic: promptguard.transactionscore.from-pp] -- \"consume msg\" --> PU[\"Proxy Updater <br> (Go)\"] PU -- \"Update proxy and reverse proxy\" --> R[\"Redis\"] Proxy mapper (Proxy to account mapping) Redis Key: promptguard:proxy-mapper:<proxy_id> Value: { \"fi_code\": \"014\", \"actual_account\": \"12e345g66f78a9b\", // hashed \"proxy_type\": \"mobile\", \"last_updated\": 1735900000 // unix timestamp } Available proxy types: Type PromptPay Proxy Type PromptGuard Proxy Type Proxy Updater Action Actual Account - account Ignore National ID NATID nat_id Update to Redis. Map nat_id with bank account. Mobile Number MSISDN mobile Update to Redis. Map mobile with bank account. Biller ID BILLERID biller_id Update to Redis. Map biller_id with bank account. E-Wallet EWALLETID wallet_id Update to Redis. The actual_account is wallet_id , Ex. wallet_id : 1400917891000 means: actual_account = 1400917891000 E-Mail EMAIL email Update to Redis. Curretly not existed in PP \u2139\ufe0f NOTE * PromptPay proxy_id is originally global unique. * Multiple E-Wallet users can have only one common bank account, except KBank QR receiving wallet. That's why, we do not aggregate the wallet_id into one translated bank account. * As per 2025 Nov, CFR has defined the fi_code for Thai e-wallet providers, as seen in bank_info.csv . * For any provider not listed there, we will use their TEPA code instead. The Proxy Updater updates the proxy mapper to Redis for every transaction, except the actual account transaction. It does not \u201ccheck then update\u201d \u2013 just always overwrite, for the fastest strategy. It also update the reverse-lookup sets, to retreive all proxies of an account. Reverse proxy mapper (Account to proxies mapping) Redis Key: promptguard:proxy-reverse:<fi_code>-<account_id> \u2139\ufe0f account_id is hashed to be matched with CFR data. Value: [ { \"proxy_id\": \"0987654321\", \"proxy_type\": \"mobile\" }, { \"proxy_id\": \"12345667891234\", \"proxy_type\": \"nat_id\" } ] Here are the relavant Redis commands: # Add proxy mapping SET promptguard:proxy-mapper:<proxy_id> <account_json> # Add reverse proxy mapping in the set SADD index:promptguard:proxy-reverse:<fi_code>-<account_id> <proxy_json> # Remove a reverse proxy in the set SREM index:promptguard:proxy-reverse:<fi_code>-<account_id> <proxy_json> # Get all proxies of an account SMEMBERS index:promptguard:proxy-reverse:<fi_code>-<account_id> Prediction Feature Updater This module consumes PromptPay transactions from Kafka and SeaweedFS, then update prediction features to Redis. There are 2 sub-modules shown as below: graph LR subgraph \"Real-time (from Kafka)\" direction LR KK[Kafka] -- \"consume msg\" --> RT_FU[\"Real-time Feature Updater<br>(Go)\"] RT_FU --> R1[\"Redis<br>key: promptguard:recent-txn:...\"] end subgraph \"Daily Batch (from SeaweedFS)\" direction LR FS[SeaweedFS] -- \"read daily files\" --> D_FU[\"Daily Feature Updater<br>(Go)\"] D_FU --> R2[\"Redis<br>key: promptguard:daily-features:...\"] end Near Real-time Features These prediction features are 10-min raw transaction data consumed from Kafka and cached on Redis. Key: promptguard:recent-txn:<fi_code>-<account_id> \u2139\ufe0f account_id is hashed to be matched with CFR data. Value: [ { \"action\": \"send\", \"amount\": 200, \"timestamp\": \"2023-01-01T00:00:00+07:00\", \"counterparty_account_id\": \"<hash_of_account_b>\", \"counterparty_fi_code\": \"bank_b\" }, { \"action\": \"receive\", \"amount\": 550, \"timestamp\": \"2024-01-01T00:00:00+07:00\", \"counterparty_account_id\": \"<hash_of_account_c>\", \"counterparty_fi_code\": \"bank_c\" } ] These features is kept with Redis ZADD (sorted set), sorting by unix timestamp (in seconds). The transaction score Kafka consumer (worker) and API server will aggregate these prediction features before calling the Core Prediction Service. Sequence Diagram This feature updater will comsume the PromptPay transaction from Kafka and add a new transaction to Redis. For each account, it must refresh 10-min TTL of the key and remove transaction records older than 10 minutes to maintain memory usage. sequenceDiagram participant RP as Real-time feature prep participant KK as Kafka Topic participant R as Redis note over KK: promptguard.transactionscore.from-pp RP->>KK: consume txn data activate RP activate KK KK-->>RP: txn data deactivate KK note over R: promptguard:recent-txn:<fi_code>-<account_id> RP->>R: Add new txn to 10-min features <br>for both sender & reciever <br> (ZADD with 10 min TTL) RP->>R: Remove sender & reciever features <br>Older than 10 min <br> (ZREMRANGEBYSCORE) deactivate RP Here are the relavant Redis commands: # Add transaction ZADD promptguard:recent-txn:<fi_code>-<account_id> <timestamp> <json> # set 10-min TTL EXPIRE promptguard:recent-txn:<fi_code>-<account_id> 600 # Remove old ones (10+ min) ZREMRANGEBYSCORE promptguard:recent-txn:<fi_code>-<account_id> -inf <now-600> # Read recent ones (10- min) ZRANGEBYSCORE promptguard:recent-txn:<fi_code>-<account_id> <now-600> +inf # Read all ZRANGE promptguard:recent-txn:<fi_code>-<account_id> 0 -1 Daily aggregated features Key: promptguard:daily-features:<fi_code>-<account_id> account_id is hashed to be matched with CFR data. Value: { \"f1\": 150, \"f2\": 500, \"f3\": 30, ... \"f300\": 300.0 } These features will be daily processed from SeaweedFS, then kept in Redis. We can use GET and SET commands for Redis without expiration set. If the cache lost, we can retrieve them from SeaweedFS. Sequence Diagram sequenceDiagram participant DDP as Daily feature prep participant SF as SeaweedFS participant R as Redis DDP->>SF: daily read txn data activate DDP activate SF SF-->>DDP: data deactivate SF DDP->>DDP: calculate daily features note over R: promptguard:daily-features:<fi_code>-<account_id> DDP->>R: save daily features DDP->>SF: save daily features deactivate DDP TODO: idempotent, prevent loss Scheduled Data Preparation These are scheduled data prep for evaluation pipeline. It composes of: Predicted score capture CFR data capture Predicted Score Capture This worker consumes score messages from Kafka topic: promptguard.transactionscore.inferred , then save to SeaweedFS for later used in model evaluation pipeline. graph LR KK[Kafka] -- \"consume msg from topic: promptguard.transactionscore.inferred\" --> SC[\"Predicted Score Capture <br> (python)\"] SC -- Append a batch of new score --> SW[\"SeaweedFS <br> (Parquet)\"] The worker use python for easily handle the data in paquete format. It will keep the predicted data in daily partitioned folder, splitted by an optimal size. The predicted data retention period is set to 13 months, for seasoning behavior understanding. We can set the SeaweedFS bucket lifecycle via this s3 command: aws --endpoint-url http://seaweedfs-s3-endpoint:8333 s3api put-bucket-lifecycle-configuration \\ --bucket my-bucket \\ --lifecycle-configuration '{ \"Rules\": [ { \"ID\": \"DeleteOldData\", \"Prefix\": \"transactionscore_inferred/\", \"Status\": \"Enabled\", \"Expiration\": { \"Days\": 397 } } ] }' CFR Data Capture This worker will be scheduled pulling anonymous CFR data from Production. The firewall between the worker and CFR networks are separated, and will be opened as agreed with GRC team. \u2139\ufe0f As per 2025 Nov, we agreed to open it every 15 day for 3 hours. graph LR OC[Oracle <br> CFR Prod] -- \"read CFR data <br> (temp firewall)\" --> CDC[\"CFR Data Capture <br> (python)\"] CDC -- Parquet store --> SW[\"SeaweedFS <br> (Parquet)\"] Estimated size of CFR data: SeaweedFS Sizing .","title":"Data Preparation"},{"location":"lab-env/data-prep/#real-time-transaction-data-preparation","text":"Real-time transaction data prep consumes message from the Kafka topic: promptguard.transactionscore.from-pp . There are 3 sub modules here: Transaction recorder: to save transactions for using in daily feature preparation and evaluation pipeline (data profile shifing check) Proxy Updater: to update bank account proxy id Prediction feature preparation: to use in prediction pipeline graph TD LS[PP Log Streamer] -- produce --> KK[Kafka <br>topic: promptguard.transactionscore.from-pp] KK -- \"consume msg\" --> TR[\"Transaction Recorder<br> (Lookup & Credit transfer)<br> (Go)\"] TR -- accumulate & save --> SF[\"SeaweedFS <br> (Parquet)\"] KK -- \"consume msg\" --> PU[\"Proxy Updater<br> (only Lookup msg)<br> (Go)\"] PU -- \"Update proxy and reverse proxy\" --> R[\"Redis\"] KK -- \"consume msg\" --> FT[\"Prediction Feature Updater<br> (only Credit transfer msg)<br> (Go)\"] FT -- \"Update account recent transactions (10-min feature)\" --> R","title":"Real-time Transaction Data Preparation"},{"location":"lab-env/data-prep/#sequence-diagram","text":"sequenceDiagram participant LE as Log extractor (Blend / Loki) participant KK as Kafka Topic participant TA as Transaction recorder participant RP as Real-time feature prep participant R as Redis participant SF as SeaweedFS participant DDP as Daily feature prep participant PUD as Proxy updater LE->>KK: publish anonym txn note over KK: promptguard.transactionscore.from-pp TA->>KK: consume txn data activate TA activate KK KK-->>TA: txn data deactivate KK TA->>TA: batch packing TA->>SF: batch data store deactivate TA PUD->>KK: consume txn data activate PUD activate KK KK-->>PUD: txn data deactivate KK PUD->>R: update proxy to Redis deactivate PUD RP->>KK: consume txn data activate RP activate KK KK-->>RP: txn data deactivate KK RP->>R: update recent txn feature deactivate RP DDP->>SF: daily read txn data activate DDP activate SF SF-->>DDP: data deactivate SF DDP->>DDP: cal daily features DDP->>R: save daily features DDP->>SF: save daily features deactivate DDP","title":"Sequence Diagram"},{"location":"lab-env/data-prep/#promptpay-transaction-recorder","text":"This module will consume the PromptPay transaction from promptguard.transactionscore.from-pp topic. It accommulates the transaction data then saves to SeaweedFS, in Parquet (Zstd) format. graph LR LS[PP Log Streamer] -- produce --> KK[Kafka <br>topic: promptguard.transactionscore.from-pp] KK -- \"consume msg\" --> TR[\"Transaction Recorder<br> (Lookup & Credit transfer)<br> (Go)\"] TR -- accumulate & save --> SF[\"SeaweedFS <br> (Parquet)\"] The transaction schema is defined here: PromptPay message schema . These saved transactions will be used by Daily Feature Preparation , Evaluation , and Training pipelines.","title":"PromptPay Transaction recorder"},{"location":"lab-env/data-prep/#proxy-updater","text":"This module consumes the PromptPay transaction from promptguard.transactionscore.from-pp topic. It extracts proxy and actual account information, if any, then update to proxy mapper in Redis. graph LR KK[Kafka <br>topic: promptguard.transactionscore.from-pp] -- \"consume msg\" --> PU[\"Proxy Updater <br> (Go)\"] PU -- \"Update proxy and reverse proxy\" --> R[\"Redis\"]","title":"Proxy Updater"},{"location":"lab-env/data-prep/#proxy-mapper-proxy-to-account-mapping","text":"Redis Key: promptguard:proxy-mapper:<proxy_id> Value: { \"fi_code\": \"014\", \"actual_account\": \"12e345g66f78a9b\", // hashed \"proxy_type\": \"mobile\", \"last_updated\": 1735900000 // unix timestamp } Available proxy types: Type PromptPay Proxy Type PromptGuard Proxy Type Proxy Updater Action Actual Account - account Ignore National ID NATID nat_id Update to Redis. Map nat_id with bank account. Mobile Number MSISDN mobile Update to Redis. Map mobile with bank account. Biller ID BILLERID biller_id Update to Redis. Map biller_id with bank account. E-Wallet EWALLETID wallet_id Update to Redis. The actual_account is wallet_id , Ex. wallet_id : 1400917891000 means: actual_account = 1400917891000 E-Mail EMAIL email Update to Redis. Curretly not existed in PP \u2139\ufe0f NOTE * PromptPay proxy_id is originally global unique. * Multiple E-Wallet users can have only one common bank account, except KBank QR receiving wallet. That's why, we do not aggregate the wallet_id into one translated bank account. * As per 2025 Nov, CFR has defined the fi_code for Thai e-wallet providers, as seen in bank_info.csv . * For any provider not listed there, we will use their TEPA code instead. The Proxy Updater updates the proxy mapper to Redis for every transaction, except the actual account transaction. It does not \u201ccheck then update\u201d \u2013 just always overwrite, for the fastest strategy. It also update the reverse-lookup sets, to retreive all proxies of an account.","title":"Proxy mapper (Proxy to account mapping)"},{"location":"lab-env/data-prep/#reverse-proxy-mapper-account-to-proxies-mapping","text":"Redis Key: promptguard:proxy-reverse:<fi_code>-<account_id> \u2139\ufe0f account_id is hashed to be matched with CFR data. Value: [ { \"proxy_id\": \"0987654321\", \"proxy_type\": \"mobile\" }, { \"proxy_id\": \"12345667891234\", \"proxy_type\": \"nat_id\" } ] Here are the relavant Redis commands: # Add proxy mapping SET promptguard:proxy-mapper:<proxy_id> <account_json> # Add reverse proxy mapping in the set SADD index:promptguard:proxy-reverse:<fi_code>-<account_id> <proxy_json> # Remove a reverse proxy in the set SREM index:promptguard:proxy-reverse:<fi_code>-<account_id> <proxy_json> # Get all proxies of an account SMEMBERS index:promptguard:proxy-reverse:<fi_code>-<account_id>","title":"Reverse proxy mapper (Account to proxies mapping)"},{"location":"lab-env/data-prep/#prediction-feature-updater","text":"This module consumes PromptPay transactions from Kafka and SeaweedFS, then update prediction features to Redis. There are 2 sub-modules shown as below: graph LR subgraph \"Real-time (from Kafka)\" direction LR KK[Kafka] -- \"consume msg\" --> RT_FU[\"Real-time Feature Updater<br>(Go)\"] RT_FU --> R1[\"Redis<br>key: promptguard:recent-txn:...\"] end subgraph \"Daily Batch (from SeaweedFS)\" direction LR FS[SeaweedFS] -- \"read daily files\" --> D_FU[\"Daily Feature Updater<br>(Go)\"] D_FU --> R2[\"Redis<br>key: promptguard:daily-features:...\"] end","title":"Prediction Feature Updater"},{"location":"lab-env/data-prep/#near-real-time-features","text":"These prediction features are 10-min raw transaction data consumed from Kafka and cached on Redis. Key: promptguard:recent-txn:<fi_code>-<account_id> \u2139\ufe0f account_id is hashed to be matched with CFR data. Value: [ { \"action\": \"send\", \"amount\": 200, \"timestamp\": \"2023-01-01T00:00:00+07:00\", \"counterparty_account_id\": \"<hash_of_account_b>\", \"counterparty_fi_code\": \"bank_b\" }, { \"action\": \"receive\", \"amount\": 550, \"timestamp\": \"2024-01-01T00:00:00+07:00\", \"counterparty_account_id\": \"<hash_of_account_c>\", \"counterparty_fi_code\": \"bank_c\" } ] These features is kept with Redis ZADD (sorted set), sorting by unix timestamp (in seconds). The transaction score Kafka consumer (worker) and API server will aggregate these prediction features before calling the Core Prediction Service.","title":"Near Real-time Features"},{"location":"lab-env/data-prep/#sequence-diagram_1","text":"This feature updater will comsume the PromptPay transaction from Kafka and add a new transaction to Redis. For each account, it must refresh 10-min TTL of the key and remove transaction records older than 10 minutes to maintain memory usage. sequenceDiagram participant RP as Real-time feature prep participant KK as Kafka Topic participant R as Redis note over KK: promptguard.transactionscore.from-pp RP->>KK: consume txn data activate RP activate KK KK-->>RP: txn data deactivate KK note over R: promptguard:recent-txn:<fi_code>-<account_id> RP->>R: Add new txn to 10-min features <br>for both sender & reciever <br> (ZADD with 10 min TTL) RP->>R: Remove sender & reciever features <br>Older than 10 min <br> (ZREMRANGEBYSCORE) deactivate RP Here are the relavant Redis commands: # Add transaction ZADD promptguard:recent-txn:<fi_code>-<account_id> <timestamp> <json> # set 10-min TTL EXPIRE promptguard:recent-txn:<fi_code>-<account_id> 600 # Remove old ones (10+ min) ZREMRANGEBYSCORE promptguard:recent-txn:<fi_code>-<account_id> -inf <now-600> # Read recent ones (10- min) ZRANGEBYSCORE promptguard:recent-txn:<fi_code>-<account_id> <now-600> +inf # Read all ZRANGE promptguard:recent-txn:<fi_code>-<account_id> 0 -1","title":"Sequence Diagram"},{"location":"lab-env/data-prep/#daily-aggregated-features","text":"Key: promptguard:daily-features:<fi_code>-<account_id> account_id is hashed to be matched with CFR data. Value: { \"f1\": 150, \"f2\": 500, \"f3\": 30, ... \"f300\": 300.0 } These features will be daily processed from SeaweedFS, then kept in Redis. We can use GET and SET commands for Redis without expiration set. If the cache lost, we can retrieve them from SeaweedFS.","title":"Daily aggregated features"},{"location":"lab-env/data-prep/#sequence-diagram_2","text":"sequenceDiagram participant DDP as Daily feature prep participant SF as SeaweedFS participant R as Redis DDP->>SF: daily read txn data activate DDP activate SF SF-->>DDP: data deactivate SF DDP->>DDP: calculate daily features note over R: promptguard:daily-features:<fi_code>-<account_id> DDP->>R: save daily features DDP->>SF: save daily features deactivate DDP TODO: idempotent, prevent loss","title":"Sequence Diagram"},{"location":"lab-env/data-prep/#scheduled-data-preparation","text":"These are scheduled data prep for evaluation pipeline. It composes of: Predicted score capture CFR data capture","title":"Scheduled Data Preparation"},{"location":"lab-env/data-prep/#predicted-score-capture","text":"This worker consumes score messages from Kafka topic: promptguard.transactionscore.inferred , then save to SeaweedFS for later used in model evaluation pipeline. graph LR KK[Kafka] -- \"consume msg from topic: promptguard.transactionscore.inferred\" --> SC[\"Predicted Score Capture <br> (python)\"] SC -- Append a batch of new score --> SW[\"SeaweedFS <br> (Parquet)\"] The worker use python for easily handle the data in paquete format. It will keep the predicted data in daily partitioned folder, splitted by an optimal size. The predicted data retention period is set to 13 months, for seasoning behavior understanding. We can set the SeaweedFS bucket lifecycle via this s3 command: aws --endpoint-url http://seaweedfs-s3-endpoint:8333 s3api put-bucket-lifecycle-configuration \\ --bucket my-bucket \\ --lifecycle-configuration '{ \"Rules\": [ { \"ID\": \"DeleteOldData\", \"Prefix\": \"transactionscore_inferred/\", \"Status\": \"Enabled\", \"Expiration\": { \"Days\": 397 } } ] }'","title":"Predicted Score Capture"},{"location":"lab-env/data-prep/#cfr-data-capture","text":"This worker will be scheduled pulling anonymous CFR data from Production. The firewall between the worker and CFR networks are separated, and will be opened as agreed with GRC team. \u2139\ufe0f As per 2025 Nov, we agreed to open it every 15 day for 3 hours. graph LR OC[Oracle <br> CFR Prod] -- \"read CFR data <br> (temp firewall)\" --> CDC[\"CFR Data Capture <br> (python)\"] CDC -- Parquet store --> SW[\"SeaweedFS <br> (Parquet)\"] Estimated size of CFR data: SeaweedFS Sizing .","title":"CFR Data Capture"},{"location":"lab-env/hlds/","text":"High Level Design This section will provide you broad vision of the system. C1 \u2013 Context C2 \u2013 Container Diagram High Level design Transaction Risk Score graph TD subgraph \"Sender Bank\" A[\"Sender Bank<br>(API Call)\"] F{\"Risk Score Decision<br>(Allow / Challenge / Block)\"} end A --> B[\"Risk Scoring API<br>(REST)\"] PPL[\"PromptPay Logging<br>(promptail -> Loki)\"] --> G[\"Kafka<br>(Transaction Logging)\"] %% Transaction Request Flow subgraph \"Score Serving\" B --> Get_cached[\"Get cached score<br>(Redis)\"] Get_cached --> CON1{\"Found?\"} E[\"API Response to Sender Bank\"] end CON1 -- Yes --> E D --> E E -- (<100ms Total Round Trip) --> F subgraph \"Real-Time Scoring Core\" FT_UD CON1 -- No --> C[\"Feature Lookup<br>(Redis)\"] C --> D[\"Model Inference<br>(Python)\"] D --> Cache_score[\"Cache Score<br>(Redis)\"] end %% Batch and Analytics subgraph \"Batch Pipeline\" H --> K[\"Model Training<br>(Python, Spark)\"] H --> L[\"Fraud Trail Analysis<br>Graph Traversals\"] K --> M[\"Model Registry<br>(Versioned Models)\"] M --> D L --> J[\"Feature Store Updates<br>(for future scoring)\"] end %% Async Processing and Logging G -- predict score --> C G -- update account profile --> FT_UD[\"Feature Update<br>(Redis)\"] G --> H[\"Data Lake<br>(Parquet)\"] %% Monitoring & MLOps %% B -.-> N[\"Prometheus / Grafana<br>Latency & Health Metrics\"] %% D -.-> N %% K -.-> O[\"Model Metrics & Drift Detection\"] %% comment below to make it easier to read (the monitor is needed both real-time and batch) %% K -.-> N %% H -.-> N %% L -.-> N style Get_cached fill:#a5a,stroke:#333,stroke-width:2px style Cache_score fill:#a5a,stroke:#333,stroke-width:2px style D fill:#88a,stroke:#333 style A fill:#595,stroke:#333 style E fill:#595,stroke:#333 style F fill:#595,stroke:#333 C3 \u2013 Component Diagram List of Components Later in this doc, we will provide more information of each component following: Blendata PromptPay fetcher Apigee API gateway ITMX SSO integration Jupyter Hub Transaction risk score service API service Core Prediction Service Redis feature store Kafka topic Real-time Transaction Data Preparation File Storage: Model Repo, Prediction Result Store Account risk score service API service Database & Data model Prediction Pipeline Redis feature File Storage: Master data, Model Repo, Prediction Result Store Data Preparation Pipeline Pipeline Spec Data for training Feature data for prediction: CFR, PromptPay Feature snapshot (for evaluation) Model Training Pipeline Pipeline Spec File Storage: Model Repo Model Evaluation Pipeline Pipeline Spec File Storage: Model Repo Prediction Result Store (transaction and account score) CFR answer Feature snapshot of existing model Current Feature data Database & Data model to keep evaluation result Grafana dashboard Fraud Hub Web spec Database File Storage C3 \u2013 Component Diagram (Production Environment) For Prod Environment, please refer to: PromptGuard-TRS-Internal-Prod Technical Specification. TODO give link \u2003 Appendix I: Best practice of fraud risk score architecture Refer to a GPT research: link graph TD %% Transaction Request Flow A[\"Sender Bank<br>(API Call)\"] --> B[\"Risk Scoring API<br>(REST/gRPC)\"] subgraph \"Real-Time Scoring Core\" B --> C[\"Feature Lookup<br>(Redis/In-Mem Cache)\"] B --> D[\"Model Inference<br>(TensorFlow Serving / Go Service)\"] C --> D D --> E{\"Risk Score Decision<br>(Allow / Challenge / Block)\"} end E --> F[\"API Response to Sender Bank<br>(<100ms Total Round Trip)\"] %% Async Processing and Logging B --> G[\"Kafka / Message Bus<br>(Transaction Logging)\"] G --> H[\"Data Lake<br>(Parquet, ClickHouse)\"] G --> I[\"Graph DB<br>(Neo4j, TigerGraph)\"] G --> J[\"Feature Store Updates<br>(for future scoring)\"] %% Batch and Analytics subgraph \"Batch Pipeline\" H --> K[\"Model Training<br>(Python, Spark)\"] I --> L[\"Fraud Ring Analysis<br>Graph Traversals\"] K --> M[\"Model Registry<br>(Versioned Models)\"] M --> D L --> J end %% Monitoring & MLOps B -.-> N[\"Prometheus / Grafana<br>Latency & Health Metrics\"] D -.-> N K -.-> O[\"Model Metrics & Drift Detection\"] %% comment below to make it easier to read (the monitor is needed both real-time and batch) %% K -.-> N %% H -.-> N %% L -.-> N style B fill:#a5a,stroke:#333,stroke-width:2px style D fill:#88a,stroke:#333 style E fill:#595,stroke:#333 style F fill:#595,stroke:#333 II: Lab Environment Hardware Specification As of 2025, we awarded Yip In Tsoi as the HW provider and BlueBik as the system intefrator of the lab environment. Here is the hardware spec and SI work scope: Hardware Spec QTY. SI Work Scope Note Lab Servers CPU: 168 Core RAM: 2560 GB GPU: Nvdia L40S 48GB RAM Storage: 2x 1.6 TB SSD SAS 24 Gbps RAID 1 OS: CentOS 3 Kubernetes deployment - Install k8s foundation (v1.32+, Containerd, Flannel, Nginx IC) - Master & Worker on all 3 nodes - UI: Kubernetes Dashboard or similar - Configure audit log to local storage Analytics Hub Deployment - Apache Airflow - PySpark - Cassandra - DBT or another data transformation tool - Other components as bidder propose Total spec CPU: 504 Core RAM: 7680 GB Storage: 9.6 TB FileSystem Server CPU: 64 Core RAM: 512 GB OS Disk: 2x 800GB SSD SAS 24 Gbps RAID1 Storage: 8x SAS HDD / 12 Gbps 7200rpm 24TB OS: CentOS 1 SeaweedFS deployment Total Storage: 192 TB * After configured parity disks, usable storage will be lower.","title":"High Level Design"},{"location":"lab-env/hlds/#high-level-design","text":"This section will provide you broad vision of the system.","title":"High Level Design"},{"location":"lab-env/hlds/#c1-context","text":"","title":"C1 \u2013 Context"},{"location":"lab-env/hlds/#c2-container-diagram","text":"","title":"C2 \u2013 Container Diagram"},{"location":"lab-env/hlds/#high-level-design_1","text":"Transaction Risk Score graph TD subgraph \"Sender Bank\" A[\"Sender Bank<br>(API Call)\"] F{\"Risk Score Decision<br>(Allow / Challenge / Block)\"} end A --> B[\"Risk Scoring API<br>(REST)\"] PPL[\"PromptPay Logging<br>(promptail -> Loki)\"] --> G[\"Kafka<br>(Transaction Logging)\"] %% Transaction Request Flow subgraph \"Score Serving\" B --> Get_cached[\"Get cached score<br>(Redis)\"] Get_cached --> CON1{\"Found?\"} E[\"API Response to Sender Bank\"] end CON1 -- Yes --> E D --> E E -- (<100ms Total Round Trip) --> F subgraph \"Real-Time Scoring Core\" FT_UD CON1 -- No --> C[\"Feature Lookup<br>(Redis)\"] C --> D[\"Model Inference<br>(Python)\"] D --> Cache_score[\"Cache Score<br>(Redis)\"] end %% Batch and Analytics subgraph \"Batch Pipeline\" H --> K[\"Model Training<br>(Python, Spark)\"] H --> L[\"Fraud Trail Analysis<br>Graph Traversals\"] K --> M[\"Model Registry<br>(Versioned Models)\"] M --> D L --> J[\"Feature Store Updates<br>(for future scoring)\"] end %% Async Processing and Logging G -- predict score --> C G -- update account profile --> FT_UD[\"Feature Update<br>(Redis)\"] G --> H[\"Data Lake<br>(Parquet)\"] %% Monitoring & MLOps %% B -.-> N[\"Prometheus / Grafana<br>Latency & Health Metrics\"] %% D -.-> N %% K -.-> O[\"Model Metrics & Drift Detection\"] %% comment below to make it easier to read (the monitor is needed both real-time and batch) %% K -.-> N %% H -.-> N %% L -.-> N style Get_cached fill:#a5a,stroke:#333,stroke-width:2px style Cache_score fill:#a5a,stroke:#333,stroke-width:2px style D fill:#88a,stroke:#333 style A fill:#595,stroke:#333 style E fill:#595,stroke:#333 style F fill:#595,stroke:#333","title":"High Level design"},{"location":"lab-env/hlds/#c3-component-diagram","text":"","title":"C3 \u2013 Component Diagram"},{"location":"lab-env/hlds/#list-of-components","text":"Later in this doc, we will provide more information of each component following: Blendata PromptPay fetcher Apigee API gateway ITMX SSO integration Jupyter Hub Transaction risk score service API service Core Prediction Service Redis feature store Kafka topic Real-time Transaction Data Preparation File Storage: Model Repo, Prediction Result Store Account risk score service API service Database & Data model Prediction Pipeline Redis feature File Storage: Master data, Model Repo, Prediction Result Store Data Preparation Pipeline Pipeline Spec Data for training Feature data for prediction: CFR, PromptPay Feature snapshot (for evaluation) Model Training Pipeline Pipeline Spec File Storage: Model Repo Model Evaluation Pipeline Pipeline Spec File Storage: Model Repo Prediction Result Store (transaction and account score) CFR answer Feature snapshot of existing model Current Feature data Database & Data model to keep evaluation result Grafana dashboard Fraud Hub Web spec Database File Storage","title":"List of Components"},{"location":"lab-env/hlds/#c3-component-diagram-production-environment","text":"For Prod Environment, please refer to: PromptGuard-TRS-Internal-Prod Technical Specification. TODO give link","title":"C3 \u2013 Component Diagram (Production Environment)"},{"location":"lab-env/hlds/#appendix","text":"","title":"Appendix"},{"location":"lab-env/hlds/#i-best-practice-of-fraud-risk-score-architecture","text":"Refer to a GPT research: link graph TD %% Transaction Request Flow A[\"Sender Bank<br>(API Call)\"] --> B[\"Risk Scoring API<br>(REST/gRPC)\"] subgraph \"Real-Time Scoring Core\" B --> C[\"Feature Lookup<br>(Redis/In-Mem Cache)\"] B --> D[\"Model Inference<br>(TensorFlow Serving / Go Service)\"] C --> D D --> E{\"Risk Score Decision<br>(Allow / Challenge / Block)\"} end E --> F[\"API Response to Sender Bank<br>(<100ms Total Round Trip)\"] %% Async Processing and Logging B --> G[\"Kafka / Message Bus<br>(Transaction Logging)\"] G --> H[\"Data Lake<br>(Parquet, ClickHouse)\"] G --> I[\"Graph DB<br>(Neo4j, TigerGraph)\"] G --> J[\"Feature Store Updates<br>(for future scoring)\"] %% Batch and Analytics subgraph \"Batch Pipeline\" H --> K[\"Model Training<br>(Python, Spark)\"] I --> L[\"Fraud Ring Analysis<br>Graph Traversals\"] K --> M[\"Model Registry<br>(Versioned Models)\"] M --> D L --> J end %% Monitoring & MLOps B -.-> N[\"Prometheus / Grafana<br>Latency & Health Metrics\"] D -.-> N K -.-> O[\"Model Metrics & Drift Detection\"] %% comment below to make it easier to read (the monitor is needed both real-time and batch) %% K -.-> N %% H -.-> N %% L -.-> N style B fill:#a5a,stroke:#333,stroke-width:2px style D fill:#88a,stroke:#333 style E fill:#595,stroke:#333 style F fill:#595,stroke:#333","title":"I: Best practice of fraud risk score architecture"},{"location":"lab-env/hlds/#ii-lab-environment-hardware-specification","text":"As of 2025, we awarded Yip In Tsoi as the HW provider and BlueBik as the system intefrator of the lab environment. Here is the hardware spec and SI work scope: Hardware Spec QTY. SI Work Scope Note Lab Servers CPU: 168 Core RAM: 2560 GB GPU: Nvdia L40S 48GB RAM Storage: 2x 1.6 TB SSD SAS 24 Gbps RAID 1 OS: CentOS 3 Kubernetes deployment - Install k8s foundation (v1.32+, Containerd, Flannel, Nginx IC) - Master & Worker on all 3 nodes - UI: Kubernetes Dashboard or similar - Configure audit log to local storage Analytics Hub Deployment - Apache Airflow - PySpark - Cassandra - DBT or another data transformation tool - Other components as bidder propose Total spec CPU: 504 Core RAM: 7680 GB Storage: 9.6 TB FileSystem Server CPU: 64 Core RAM: 512 GB OS Disk: 2x 800GB SSD SAS 24 Gbps RAID1 Storage: 8x SAS HDD / 12 Gbps 7200rpm 24TB OS: CentOS 1 SeaweedFS deployment Total Storage: 192 TB * After configured parity disks, usable storage will be lower.","title":"II: Lab Environment Hardware Specification"},{"location":"lab-env/intro/","text":"Introduction Purpose This document is designed to provide standard specification to ITMX internal to further developing PromptGuard Transaction Risk Score service which will include following topics: 1. TODO 2. TODO 3. TODO 4. TODO Definition This document will use the following definition: Section Code Meaning Description Institute ITMX National ITMX National Interbank Transaction Management and Exchange MBs Member Bank(s) \u0e18\u0e19\u0e32\u0e04\u0e32\u0e23\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01 \u0e43\u0e19\u0e42\u0e04\u0e23\u0e07\u0e01\u0e32\u0e23 CFR, \u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e20\u0e32\u0e22\u0e43\u0e15\u0e49\u0e01\u0e32\u0e23\u0e01\u0e33\u0e01\u0e31\u0e1a, \u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 Non-bank Non-bank \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19, \u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 (Non-bank) FI, FIs Financial institute(s) \u0e18\u0e19\u0e32\u0e04\u0e32\u0e23\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e41\u0e25\u0e30\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 (non-bank) \u0e43\u0e19\u0e42\u0e04\u0e23\u0e07\u0e01\u0e32\u0e23 CFR \u0e43\u0e19\u0e1a\u0e32\u0e07\u0e1a\u0e23\u0e34\u0e1a\u0e17 \u0e08\u0e30\u0e2b\u0e21\u0e32\u0e22\u0e16\u0e36\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 (non-bank) \u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e33\u0e04\u0e31\u0e0d System CFR Central Fraud Registry Central Fraud Registry PP PromptPay \u0e23\u0e30\u0e1a\u0e1a\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e40\u0e1e\u0e22\u0e4c \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e42\u0e14\u0e22 ITMX Field M Mandatory The data element is mandatory and must be provided in the message C Condition The data element is optional and may be provided by the message originator with condition O Optional The data element is optional and may be provided in the message or do not N Not applicable This field must not be filled in. It is either not required in this context or should be left blank because providing a value may cause validation issues or is not permitted by system rules. X Not available This field does not exist in this template/message P Data from Police The data element is required, and system will not detect, not validate by ignore field value Due to CCIB system will provide information instead of manual input information from user S Data recorded from system The data element is required, and system will not detect, not validate by ignore field value Due to CFR system will provide information instead of manual input information from user MR Match request or reply Value matches the request value. N/A Not available This field does not available for this particular type of message REQ Request Sender source will send some data to destination RES Response Destination will return the result of request back to the sender source [1,1] One to One The field is required and must be included in the payload. Its value may be empty, but the field itself must still be provided. [0..1] Zero to One Optional data, limits only one item on the message In JSON payload, in case of no data, do not send field name and value [0..0] Zero to Zero No need to send data [0..M] Zero to Many Optional data and there is no limit on the message In JSON payload, in case of no data, do not send field name and value. In case of this system the maximum limit is 100 [1..M] One to Many This field must be present in the message and contain at least one value. There is no upper limit on the number of items. In case of this system the maximum limit is 100","title":"Introduction"},{"location":"lab-env/intro/#introduction","text":"","title":"Introduction"},{"location":"lab-env/intro/#purpose","text":"This document is designed to provide standard specification to ITMX internal to further developing PromptGuard Transaction Risk Score service which will include following topics: 1. TODO 2. TODO 3. TODO 4. TODO","title":"Purpose"},{"location":"lab-env/intro/#definition","text":"This document will use the following definition: Section Code Meaning Description Institute ITMX National ITMX National Interbank Transaction Management and Exchange MBs Member Bank(s) \u0e18\u0e19\u0e32\u0e04\u0e32\u0e23\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01 \u0e43\u0e19\u0e42\u0e04\u0e23\u0e07\u0e01\u0e32\u0e23 CFR, \u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e20\u0e32\u0e22\u0e43\u0e15\u0e49\u0e01\u0e32\u0e23\u0e01\u0e33\u0e01\u0e31\u0e1a, \u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 Non-bank Non-bank \u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19, \u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 (Non-bank) FI, FIs Financial institute(s) \u0e18\u0e19\u0e32\u0e04\u0e32\u0e23\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e41\u0e25\u0e30\u0e2a\u0e21\u0e32\u0e0a\u0e34\u0e01\u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 (non-bank) \u0e43\u0e19\u0e42\u0e04\u0e23\u0e07\u0e01\u0e32\u0e23 CFR \u0e43\u0e19\u0e1a\u0e32\u0e07\u0e1a\u0e23\u0e34\u0e1a\u0e17 \u0e08\u0e30\u0e2b\u0e21\u0e32\u0e22\u0e16\u0e36\u0e07\u0e1c\u0e39\u0e49\u0e43\u0e2b\u0e49\u0e1a\u0e23\u0e34\u0e01\u0e32\u0e23\u0e17\u0e32\u0e07\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19\u0e17\u0e35\u0e48\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e48\u0e2a\u0e16\u0e32\u0e1a\u0e31\u0e19\u0e01\u0e32\u0e23\u0e40\u0e07\u0e34\u0e19 (non-bank) \u0e40\u0e1b\u0e47\u0e19\u0e2a\u0e33\u0e04\u0e31\u0e0d System CFR Central Fraud Registry Central Fraud Registry PP PromptPay \u0e23\u0e30\u0e1a\u0e1a\u0e1e\u0e23\u0e49\u0e2d\u0e21\u0e40\u0e1e\u0e22\u0e4c \u0e1a\u0e23\u0e34\u0e2b\u0e32\u0e23\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e42\u0e14\u0e22 ITMX Field M Mandatory The data element is mandatory and must be provided in the message C Condition The data element is optional and may be provided by the message originator with condition O Optional The data element is optional and may be provided in the message or do not N Not applicable This field must not be filled in. It is either not required in this context or should be left blank because providing a value may cause validation issues or is not permitted by system rules. X Not available This field does not exist in this template/message P Data from Police The data element is required, and system will not detect, not validate by ignore field value Due to CCIB system will provide information instead of manual input information from user S Data recorded from system The data element is required, and system will not detect, not validate by ignore field value Due to CFR system will provide information instead of manual input information from user MR Match request or reply Value matches the request value. N/A Not available This field does not available for this particular type of message REQ Request Sender source will send some data to destination RES Response Destination will return the result of request back to the sender source [1,1] One to One The field is required and must be included in the payload. Its value may be empty, but the field itself must still be provided. [0..1] Zero to One Optional data, limits only one item on the message In JSON payload, in case of no data, do not send field name and value [0..0] Zero to Zero No need to send data [0..M] Zero to Many Optional data and there is no limit on the message In JSON payload, in case of no data, do not send field name and value. In case of this system the maximum limit is 100 [1..M] One to Many This field must be present in the message and contain at least one value. There is no upper limit on the number of items. In case of this system the maximum limit is 100","title":"Definition"},{"location":"lab-env/version-ctrl/","text":"Fraud Engine Versioning Control PromptGuard is designed to self maintaining in term of model performance. It will periodically evaluate and update prediction models, and also serving the risk score. This section show how we control these modules versioning control. TODO ... Update new model, existing features Upgrade new model and NEW features relevant components we need no downtime for updrading a model rolling update strategy how long we need for blue/green switch? Adhoc sizing for upgrade model","title":"Fraud Engine Version Control"},{"location":"lab-env/version-ctrl/#fraud-engine-versioning-control","text":"PromptGuard is designed to self maintaining in term of model performance. It will periodically evaluate and update prediction models, and also serving the risk score. This section show how we control these modules versioning control.","title":"Fraud Engine Versioning Control"},{"location":"lab-env/version-ctrl/#todo","text":"... Update new model, existing features Upgrade new model and NEW features relevant components we need no downtime for updrading a model rolling update strategy how long we need for blue/green switch? Adhoc sizing for upgrade model","title":"TODO"},{"location":"lab-env/common-components/airflow/","text":"Apache Airflow Detail\u2026 Architecture \u2026 Config \u2026","title":"Apache Airflow"},{"location":"lab-env/common-components/airflow/#apache-airflow","text":"Detail\u2026","title":"Apache Airflow"},{"location":"lab-env/common-components/airflow/#architecture","text":"\u2026","title":"Architecture"},{"location":"lab-env/common-components/airflow/#config","text":"\u2026","title":"Config"},{"location":"lab-env/common-components/kafka/","text":"Kafka Detail\u2026 Architecture \u2026 Config \u2026 Topic naming convention To support Purpose Clarity, Direction, Msg type, Stage, and Auditable, here is the key naming used entire the project: * Pattern: <domain or service>.<message type>.<stage>.[<segment>] * Any part can use dash '-' as a word delimeter, e.g., this-is-a-dataset Example: promptguard.transactionscore.from-pp promptguard.transactionscore.infered","title":"Kafka"},{"location":"lab-env/common-components/kafka/#kafka","text":"Detail\u2026","title":"Kafka"},{"location":"lab-env/common-components/kafka/#architecture","text":"\u2026","title":"Architecture"},{"location":"lab-env/common-components/kafka/#config","text":"\u2026","title":"Config"},{"location":"lab-env/common-components/kafka/#topic-naming-convention","text":"To support Purpose Clarity, Direction, Msg type, Stage, and Auditable, here is the key naming used entire the project: * Pattern: <domain or service>.<message type>.<stage>.[<segment>] * Any part can use dash '-' as a word delimeter, e.g., this-is-a-dataset","title":"Topic naming convention"},{"location":"lab-env/common-components/kafka/#example","text":"promptguard.transactionscore.from-pp promptguard.transactionscore.infered","title":"Example:"},{"location":"lab-env/common-components/postgres/","text":"Postgres DB TODO Detail\u2026 Architecture \u2026 Config \u2026","title":"Postgres"},{"location":"lab-env/common-components/postgres/#postgres-db","text":"TODO Detail\u2026","title":"Postgres DB"},{"location":"lab-env/common-components/postgres/#architecture","text":"\u2026","title":"Architecture"},{"location":"lab-env/common-components/postgres/#config","text":"\u2026","title":"Config"},{"location":"lab-env/common-components/redis/","text":"Redis PrompGuard architecture is separates into real-time and batch processing parts. The real-time part mostly use Redis to make it fast and flexible. To make PrompGuard fast and maintain an acceptable RTO level, a proper redis persistance config is needed. Use Redis Cluster as primary store, AOF everysec , maxmemory ~90 GB , noeviction , with 8 masters + 8 replicas. Key Assumptions Primary data store (not just cache) for a write-heavy workload Writes: 2,000\u20135,000 TPS (inserts/updates) Reads: 10\u201350% of writes Some data loss acceptable (can regenerate within ~1s) RAM budget: up to 2 TB. Redis storage requirement: ~500 GB Account Proxy: 150 GB Prediction Feature: 200 GB Cached predicted risk score: 80 GB Based calculation Prediction feature sizing As of Dec 2025 10-min raw transaction Estimated size 5 txns x 240 bytes x 80 M accounts ~90 GB Redis overhead ~22GB Total ~112 GB Statistic of 10-min account txn: mean: 1.33 txn, median: 1, and sd: 2.61 Daily feature Estimated size 300 features of float x 80 M accounts ~90 GB Account Proxy | Account Proxy | Estimated value | |---|---| | Base data from Oracle | 50 GB | | Redis overhead | 2x\u20133x (due to keys, pointers, encoding) | | Total | 150 GB RAM | Predicted Risk Score Predicted Risk Score Estimated value A score size (byte) (message 150 + overhead 100) 250 Avg txn per 10-min 1.33 Cache for (minute) 60 Total account 80 M Total size (GB) 80 Architecture & Sizing Cluster mode (Redis Cluster) & Replication for HA Shard keys across multiple masters. At least 1 replica per master \u2192 automatic failover \u2192 total Redis RAM budget \u2248 1.3\u20131.5 TB Master maxmemory total \u2248 650\u2013750 GB (to safely hold 500 GB without living on the edge) High-level Redis Design Redis Cluster Topology 8 masters + 8 replicas (16 nodes total) Why 8? Good balance of shard size, write throughput, and operational flexibility (resharding, maintenance, failover). Memory plan Per node RAM: 128 GB Per node maxmemory : 90 GB Masters total: 8 \u00d7 90 = 720 GB Enough headroom above your 500 GB requirement Replicas mirror that \u2192 total Redis memory reserved \u2248 1.44 TB Hardware / ops considerations Disk: NVMe SSD strongly recommended (AOF rewrite + fsync patterns) Network: 10/25GbE preferred (cluster gossip + replication + client traffic) Placement: put each master and its replica in different physical hosts / racks (anti-affinity) Client: must use cluster-aware clients/drivers Backups: periodically copy AOF/RDB off-host if you want disaster recovery Durability & Data Loss AOF everysec \u2192 at most ~1 second of data loss if crash happens. Optional: periodic off-box backups (copy AOF/RDB to object storage). Core Redis Config Recommendations PER NODE recommended config. # Memory maxmemory 90gb # Leaves headroom for OS + overhead maxmemory-policy noeviction # Primary store: better to fail than silently drop keys # Persistence (AOF) appendonly yes appendfilename \"appendonly.aof\" appendfsync everysec # Good tradeoff: fast + at most ~1s data loss no-appendfsync-on-rewrite yes # Reduce latency spikes during AOF rewrite # RDB (optional, for faster recovery) save 900 1 save 300 10 save 60 10000 # Example; adjust or disable if AOF-only is fine # Replication (example) replica-read-only yes # On replicas: # replicaof <master-host> <port> # General performance tcp-keepalive 300 timeout 0 # Don\u2019t drop idle connections protected-mode yes # Always secure with proper binding/auth Key naming convention Here is the key naming used entire the project: * Pattern: < service_or_project_name >:< dataset >:[< id >] * Any part can use dash '-' as a word delimeter, e.g., this-is-a-dataset Example: * promptguard:transactionscore:1234567890 * promptguard:feature-daily:202511 * promptguard:feature-realtime Monitoring & Ops (Key Considerations) Monitor at least : used_memory , used_memory_rss , mem_fragmentation_ratio instantaneous_ops_per_sec aof_current_size , aof_rewrite_in_progress Replication lag, cluster state Alert when: Memory > ~80\u201385% of maxmemory Replicas are out of sync AOF rewrite takes too long or fails","title":"Redis"},{"location":"lab-env/common-components/redis/#redis","text":"PrompGuard architecture is separates into real-time and batch processing parts. The real-time part mostly use Redis to make it fast and flexible. To make PrompGuard fast and maintain an acceptable RTO level, a proper redis persistance config is needed. Use Redis Cluster as primary store, AOF everysec , maxmemory ~90 GB , noeviction , with 8 masters + 8 replicas.","title":"Redis"},{"location":"lab-env/common-components/redis/#key-assumptions","text":"Primary data store (not just cache) for a write-heavy workload Writes: 2,000\u20135,000 TPS (inserts/updates) Reads: 10\u201350% of writes Some data loss acceptable (can regenerate within ~1s) RAM budget: up to 2 TB. Redis storage requirement: ~500 GB Account Proxy: 150 GB Prediction Feature: 200 GB Cached predicted risk score: 80 GB","title":"Key Assumptions"},{"location":"lab-env/common-components/redis/#based-calculation","text":"Prediction feature sizing As of Dec 2025 10-min raw transaction Estimated size 5 txns x 240 bytes x 80 M accounts ~90 GB Redis overhead ~22GB Total ~112 GB Statistic of 10-min account txn: mean: 1.33 txn, median: 1, and sd: 2.61 Daily feature Estimated size 300 features of float x 80 M accounts ~90 GB Account Proxy | Account Proxy | Estimated value | |---|---| | Base data from Oracle | 50 GB | | Redis overhead | 2x\u20133x (due to keys, pointers, encoding) | | Total | 150 GB RAM | Predicted Risk Score Predicted Risk Score Estimated value A score size (byte) (message 150 + overhead 100) 250 Avg txn per 10-min 1.33 Cache for (minute) 60 Total account 80 M Total size (GB) 80","title":"Based calculation"},{"location":"lab-env/common-components/redis/#architecture-sizing","text":"Cluster mode (Redis Cluster) & Replication for HA Shard keys across multiple masters. At least 1 replica per master \u2192 automatic failover \u2192 total Redis RAM budget \u2248 1.3\u20131.5 TB Master maxmemory total \u2248 650\u2013750 GB (to safely hold 500 GB without living on the edge)","title":"Architecture &amp; Sizing"},{"location":"lab-env/common-components/redis/#high-level-redis-design","text":"","title":"High-level Redis Design"},{"location":"lab-env/common-components/redis/#redis-cluster-topology","text":"8 masters + 8 replicas (16 nodes total) Why 8? Good balance of shard size, write throughput, and operational flexibility (resharding, maintenance, failover).","title":"Redis Cluster Topology"},{"location":"lab-env/common-components/redis/#memory-plan","text":"Per node RAM: 128 GB Per node maxmemory : 90 GB Masters total: 8 \u00d7 90 = 720 GB Enough headroom above your 500 GB requirement Replicas mirror that \u2192 total Redis memory reserved \u2248 1.44 TB","title":"Memory plan"},{"location":"lab-env/common-components/redis/#hardware-ops-considerations","text":"Disk: NVMe SSD strongly recommended (AOF rewrite + fsync patterns) Network: 10/25GbE preferred (cluster gossip + replication + client traffic) Placement: put each master and its replica in different physical hosts / racks (anti-affinity) Client: must use cluster-aware clients/drivers Backups: periodically copy AOF/RDB off-host if you want disaster recovery","title":"Hardware / ops considerations"},{"location":"lab-env/common-components/redis/#durability-data-loss","text":"AOF everysec \u2192 at most ~1 second of data loss if crash happens. Optional: periodic off-box backups (copy AOF/RDB to object storage).","title":"Durability &amp; Data Loss"},{"location":"lab-env/common-components/redis/#core-redis-config-recommendations","text":"PER NODE recommended config. # Memory maxmemory 90gb # Leaves headroom for OS + overhead maxmemory-policy noeviction # Primary store: better to fail than silently drop keys # Persistence (AOF) appendonly yes appendfilename \"appendonly.aof\" appendfsync everysec # Good tradeoff: fast + at most ~1s data loss no-appendfsync-on-rewrite yes # Reduce latency spikes during AOF rewrite # RDB (optional, for faster recovery) save 900 1 save 300 10 save 60 10000 # Example; adjust or disable if AOF-only is fine # Replication (example) replica-read-only yes # On replicas: # replicaof <master-host> <port> # General performance tcp-keepalive 300 timeout 0 # Don\u2019t drop idle connections protected-mode yes # Always secure with proper binding/auth","title":"Core Redis Config Recommendations"},{"location":"lab-env/common-components/redis/#key-naming-convention","text":"Here is the key naming used entire the project: * Pattern: < service_or_project_name >:< dataset >:[< id >] * Any part can use dash '-' as a word delimeter, e.g., this-is-a-dataset Example: * promptguard:transactionscore:1234567890 * promptguard:feature-daily:202511 * promptguard:feature-realtime","title":"Key naming convention"},{"location":"lab-env/common-components/redis/#monitoring-ops-key-considerations","text":"Monitor at least : used_memory , used_memory_rss , mem_fragmentation_ratio instantaneous_ops_per_sec aof_current_size , aof_rewrite_in_progress Replication lag, cluster state Alert when: Memory > ~80\u201385% of maxmemory Replicas are out of sync AOF rewrite takes too long or fails","title":"Monitoring &amp; Ops (Key Considerations)"},{"location":"lab-env/common-components/seaweed/","text":"SeaweedFS Detail\u2026 Architecture \u2026 Sizing SeaweedFS simply stores the Parquet files as binary blobs. It does not add extra overhead and can actually provide additional storage efficiency if we enable Erasure Coding (EC) at the SeaweedFS level, as data retention policy. The primary data kept in SeaweedFS are: Raw PromptPay transaction (PII-safe) CFR archival data (PII-safe) Predicted results with their features Transaction risk score Account risk score ML model repository Master data, e.g., FI code, Bank Account Data Analytics (Lab only) Summary Table Dataset Estimated size (13-month retention) Raw PromptPay transaction (PII-safe) 26 TB CFR archival data (PII-safe) 400 GB Predicted results with their features 28 TB ML model repository 400 MB Master data 1 GB Backup Storage 140 GB Data Analytics (Lab only) 60 TB Total ~115 TB Base Estimation Any data that kept in Parquet format will be compressed with Polars using the Zstd (Level 3) algorithm. Why? Feature Snappy Zstd (Level 3) Write (CPU) Blazing fast, low overhead Moderate; tunable levels Write (Network) Slower due to larger file size Faster due to ~39% smaller files Decompression ~3.5 GB/s ~1.0 GB/s Storage Cost Higher (~70-80% raw compressed) Significant savings (~80-90% raw compressed) Claimed 30-40% saver compare with Snappy for Parquet files Verdict: In 2025, Zstd (level 3) is the superior default because its storage and network savings far outweigh the raw CPU speed advantage of Snappy. From now on, we will use 80% compression level for any Parquet storage sizing. Raw PromptPay transaction (PII-safe) Based on Jun 2024, table below shows transaction stats: PP stats of monthly transaction Value Total account 83,962,241 Avg #of an account txn 31.54 Median 12 Max 374 \u26a0\ufe0f After cut top 1% outlier off. PP transaction storage sizing Value A message size. See: schema . ~750 bytes (raw JSON) Expected TPS 5,000 Transactions per month ~13.18 billion Retention 13 months Raw size per month ~9.88 TB Parquet size per month (Zstd) ~1.98 TB Total Parquet size (13 months) ~26 TB \u2139\ufe0f A PromptPay transaction can be either Account Lookup or Credit transfer message. CFR archival data (PII-safe) Value Today parquet dump size 25 GB Growth rate 4% / month Retention 6 months 5-year space require ~268 GB Total w headrtoom 400 GB Predicted results with their features Transaction risk score Estimated value A score size (score 4 bytes + id 18 bytes + epoch time 4 bytes) 26 bytes 500 features of float 2,000 bytes Avg txn per month 50 records Total account 100 M Retention period 13 months Total Raw 137 TB Total Parquet size (ZSTD) 27 TB TODO: Update how we do AC risk score For now, account risk score maybe derived from txn risk score, so the txn score is its feature. Account risk score Estimated value A score size (score 4 bytes + id 18 bytes + epoch time 4 bytes) 26 bytes Total account 100 M Retention period 13 months Total Raw 6.17 TB Total Parquet size (ZSTD) 1 TB ML model repository XGBoost model is very small. It does not require much space. Account risk score Estimated value Max model size 10 MB Max trained model per month 8 models Retention period 24 months Total JSON 2 GB Total Parquet size (ZSTD) 400 MB Master data There will be some small set of master data used, e.g., Bank Account info. Let's spare 1 GB for this. Backup Storage We may need to periodical backup Redis data for 2 versions. | Redis data | Estimated value | |---|---| | Account Proxy | 150 GB | | Prediction Feature | 200 GB | | Total Raw x2 | 700 GB | | Total Parquet size (ZSTD) | 140 GB | Playground Storage 60 TB Config Total storage: 192 TB Parity disk: 3 Usable storage: 120 TB","title":"SeaweedFS"},{"location":"lab-env/common-components/seaweed/#seaweedfs","text":"Detail\u2026","title":"SeaweedFS"},{"location":"lab-env/common-components/seaweed/#architecture","text":"\u2026","title":"Architecture"},{"location":"lab-env/common-components/seaweed/#sizing","text":"SeaweedFS simply stores the Parquet files as binary blobs. It does not add extra overhead and can actually provide additional storage efficiency if we enable Erasure Coding (EC) at the SeaweedFS level, as data retention policy. The primary data kept in SeaweedFS are: Raw PromptPay transaction (PII-safe) CFR archival data (PII-safe) Predicted results with their features Transaction risk score Account risk score ML model repository Master data, e.g., FI code, Bank Account Data Analytics (Lab only)","title":"Sizing"},{"location":"lab-env/common-components/seaweed/#summary-table","text":"Dataset Estimated size (13-month retention) Raw PromptPay transaction (PII-safe) 26 TB CFR archival data (PII-safe) 400 GB Predicted results with their features 28 TB ML model repository 400 MB Master data 1 GB Backup Storage 140 GB Data Analytics (Lab only) 60 TB Total ~115 TB","title":"Summary Table"},{"location":"lab-env/common-components/seaweed/#base-estimation","text":"Any data that kept in Parquet format will be compressed with Polars using the Zstd (Level 3) algorithm. Why? Feature Snappy Zstd (Level 3) Write (CPU) Blazing fast, low overhead Moderate; tunable levels Write (Network) Slower due to larger file size Faster due to ~39% smaller files Decompression ~3.5 GB/s ~1.0 GB/s Storage Cost Higher (~70-80% raw compressed) Significant savings (~80-90% raw compressed) Claimed 30-40% saver compare with Snappy for Parquet files Verdict: In 2025, Zstd (level 3) is the superior default because its storage and network savings far outweigh the raw CPU speed advantage of Snappy. From now on, we will use 80% compression level for any Parquet storage sizing. Raw PromptPay transaction (PII-safe) Based on Jun 2024, table below shows transaction stats: PP stats of monthly transaction Value Total account 83,962,241 Avg #of an account txn 31.54 Median 12 Max 374 \u26a0\ufe0f After cut top 1% outlier off. PP transaction storage sizing Value A message size. See: schema . ~750 bytes (raw JSON) Expected TPS 5,000 Transactions per month ~13.18 billion Retention 13 months Raw size per month ~9.88 TB Parquet size per month (Zstd) ~1.98 TB Total Parquet size (13 months) ~26 TB \u2139\ufe0f A PromptPay transaction can be either Account Lookup or Credit transfer message. CFR archival data (PII-safe) Value Today parquet dump size 25 GB Growth rate 4% / month Retention 6 months 5-year space require ~268 GB Total w headrtoom 400 GB Predicted results with their features Transaction risk score Estimated value A score size (score 4 bytes + id 18 bytes + epoch time 4 bytes) 26 bytes 500 features of float 2,000 bytes Avg txn per month 50 records Total account 100 M Retention period 13 months Total Raw 137 TB Total Parquet size (ZSTD) 27 TB TODO: Update how we do AC risk score For now, account risk score maybe derived from txn risk score, so the txn score is its feature. Account risk score Estimated value A score size (score 4 bytes + id 18 bytes + epoch time 4 bytes) 26 bytes Total account 100 M Retention period 13 months Total Raw 6.17 TB Total Parquet size (ZSTD) 1 TB ML model repository XGBoost model is very small. It does not require much space. Account risk score Estimated value Max model size 10 MB Max trained model per month 8 models Retention period 24 months Total JSON 2 GB Total Parquet size (ZSTD) 400 MB Master data There will be some small set of master data used, e.g., Bank Account info. Let's spare 1 GB for this. Backup Storage We may need to periodical backup Redis data for 2 versions. | Redis data | Estimated value | |---|---| | Account Proxy | 150 GB | | Prediction Feature | 200 GB | | Total Raw x2 | 700 GB | | Total Parquet size (ZSTD) | 140 GB | Playground Storage 60 TB","title":"Base Estimation"},{"location":"lab-env/common-components/seaweed/#config","text":"Total storage: 192 TB Parity disk: 3 Usable storage: 120 TB","title":"Config"},{"location":"lab-env/model-eval/","text":"Model Evaluation Pipeline PromptGuard is designed to self maintaining in term of model performance. It will periodically evaluate and update prediction models. We can evaluate both model performance and how much users follow our score suggestion (trust the score). To evaluate how user trust our scores, we can use high risk score PromptPay proxy lookup vs credit transfer transaction. If they believe the score, there will be less credit transfer transaction than the lookup. But later on in this doc, we will focus on how to evaluate the model performance. Conceptual flow Below is a conceptual flow of the model evaluation pipeline and performance maintainance. flowchart TD T_auto([Schedule Trigger]) --> SD[Check PP data statistical diff] T_manual([Manual Trigger]) --> SD T_auto --> find_new[Regulary train new challenger models] find_new --> chal_model[new model w current features] SD --> cond_sd_diff{diif > threshold \u03b1} cond_sd_diff -- No --> ending([End]) cond_sd_diff -- Yes --> noti@{ shape: display, label: \"Notify to admin\"} T_auto([Schedule Trigger]) --> model_eval[Test model performance] T_manual([Manual Trigger]) --> model_eval model_eval --> cond_model_diff{performance < threshold \u03b2} cond_model_diff -- No --> ending cond_model_diff -- Yes --> noti noti -- Admin take action --> cond_new_feature{Use current or new features?} cond_new_feature -- New --> train_new[Train new feature model offline] cond_new_feature -- Current --> chal_model chal_model -->replace[Replace old model] train_new --> new_pipe[Deploy new data pipeline] new_pipe --> replace replace --> keep_model[Keep top models for N months] keep_model --> ending Design principles: - For the current features used, the model will be shadowing regulary trained. To make sure, we always have a better model standing by. - The model retention period or number can be configurable, to make sure, we can recover from a previous model version, if needed. - All change decisions are made by human. Note that: Any feature change will impact both model and feature preparation. Flows Here we show the sequential diagrams of the evaluation pipeline: sequenceDiagram participant EV as Evaluation Pipeline participant TRN as Training Pipeline participant TXN as PP TXN participant MDL as Model Repo participant PDR as Predicted Result participant CFR as Answer from CFR participant FT as Feature Store participant DB as Eval DB TRN->>TRN: regulary train new challenger models TRN->>MDL: save challenger models <br> and their corresponse data set to repo note over EV,MDL: check statistical diff EV->>TXN: get PP model trained data <br> and up-to-date data activate EV activate TXN TXN-->>EV: PP transaction data deactivate TXN EV->>EV: check statistical diff EV->>DB: save statistical diff alt diff > threshold EV->>EV: notify to admin <br> (to maintain model performance and features) EV->>EV: check model performance drop (below section) deactivate EV end note over EV,CFR: check model performance drop EV->>PDR: get historical predicted result of the current model activate EV activate PDR PDR-->>EV: predicted result deactivate PDR EV->>CFR: get actual answer from CFR activate CFR CFR-->>EV: actual answer deactivate CFR EV->>EV: check model performance drop EV->>DB: save model performance alt drop > threshold EV->>MDL: get best challenger model activate MDL MDL-->>EV: challenger model deactivate MDL EV->>FT: get feature snapshot of the current model predicted activate FT FT-->>EV: feature snapshot deactivate FT EV->>EV: try these features with the challenger model, <br> then compare performance with the current model EV->>EV: notify admin <br> (to know the drop <br> or deploy new model) deactivate EV end \u26a0\ufe0f As of 2025, actual answer from CFR is delay around 21 days to complete for the invesment fraud cases, and 3 days for others. Apache Airflow pipelines TODO detail of the Airflow Data Prep workers These are data preparation workers saving data to SeaweedFS for evaluation pipeline. PromptPay Transaction Recorder. Detail Predicted Score Capture. Detail CFR Data Capture. Detail","title":"Model Evaluation Pipeline"},{"location":"lab-env/model-eval/#model-evaluation-pipeline","text":"PromptGuard is designed to self maintaining in term of model performance. It will periodically evaluate and update prediction models. We can evaluate both model performance and how much users follow our score suggestion (trust the score). To evaluate how user trust our scores, we can use high risk score PromptPay proxy lookup vs credit transfer transaction. If they believe the score, there will be less credit transfer transaction than the lookup. But later on in this doc, we will focus on how to evaluate the model performance.","title":"Model Evaluation Pipeline"},{"location":"lab-env/model-eval/#conceptual-flow","text":"Below is a conceptual flow of the model evaluation pipeline and performance maintainance. flowchart TD T_auto([Schedule Trigger]) --> SD[Check PP data statistical diff] T_manual([Manual Trigger]) --> SD T_auto --> find_new[Regulary train new challenger models] find_new --> chal_model[new model w current features] SD --> cond_sd_diff{diif > threshold \u03b1} cond_sd_diff -- No --> ending([End]) cond_sd_diff -- Yes --> noti@{ shape: display, label: \"Notify to admin\"} T_auto([Schedule Trigger]) --> model_eval[Test model performance] T_manual([Manual Trigger]) --> model_eval model_eval --> cond_model_diff{performance < threshold \u03b2} cond_model_diff -- No --> ending cond_model_diff -- Yes --> noti noti -- Admin take action --> cond_new_feature{Use current or new features?} cond_new_feature -- New --> train_new[Train new feature model offline] cond_new_feature -- Current --> chal_model chal_model -->replace[Replace old model] train_new --> new_pipe[Deploy new data pipeline] new_pipe --> replace replace --> keep_model[Keep top models for N months] keep_model --> ending Design principles: - For the current features used, the model will be shadowing regulary trained. To make sure, we always have a better model standing by. - The model retention period or number can be configurable, to make sure, we can recover from a previous model version, if needed. - All change decisions are made by human. Note that: Any feature change will impact both model and feature preparation.","title":"Conceptual flow"},{"location":"lab-env/model-eval/#flows","text":"Here we show the sequential diagrams of the evaluation pipeline: sequenceDiagram participant EV as Evaluation Pipeline participant TRN as Training Pipeline participant TXN as PP TXN participant MDL as Model Repo participant PDR as Predicted Result participant CFR as Answer from CFR participant FT as Feature Store participant DB as Eval DB TRN->>TRN: regulary train new challenger models TRN->>MDL: save challenger models <br> and their corresponse data set to repo note over EV,MDL: check statistical diff EV->>TXN: get PP model trained data <br> and up-to-date data activate EV activate TXN TXN-->>EV: PP transaction data deactivate TXN EV->>EV: check statistical diff EV->>DB: save statistical diff alt diff > threshold EV->>EV: notify to admin <br> (to maintain model performance and features) EV->>EV: check model performance drop (below section) deactivate EV end note over EV,CFR: check model performance drop EV->>PDR: get historical predicted result of the current model activate EV activate PDR PDR-->>EV: predicted result deactivate PDR EV->>CFR: get actual answer from CFR activate CFR CFR-->>EV: actual answer deactivate CFR EV->>EV: check model performance drop EV->>DB: save model performance alt drop > threshold EV->>MDL: get best challenger model activate MDL MDL-->>EV: challenger model deactivate MDL EV->>FT: get feature snapshot of the current model predicted activate FT FT-->>EV: feature snapshot deactivate FT EV->>EV: try these features with the challenger model, <br> then compare performance with the current model EV->>EV: notify admin <br> (to know the drop <br> or deploy new model) deactivate EV end \u26a0\ufe0f As of 2025, actual answer from CFR is delay around 21 days to complete for the invesment fraud cases, and 3 days for others.","title":"Flows"},{"location":"lab-env/model-eval/#apache-airflow-pipelines","text":"TODO detail of the Airflow","title":"Apache Airflow pipelines"},{"location":"lab-env/model-eval/#data-prep-workers","text":"These are data preparation workers saving data to SeaweedFS for evaluation pipeline. PromptPay Transaction Recorder. Detail Predicted Score Capture. Detail CFR Data Capture. Detail","title":"Data Prep workers"},{"location":"lab-env/model-train/","text":"Model Training Pipeline TODO C3 \u2013 Component Diagram Conceptual flow Below is a conceptual flow of the model training pipeline. ... Design principles: - For the current features used, the model will be shadowing regulary trained. To make sure, we always have a better model standing by. - The model retention period or number can be configurable, to make sure, we can recover from a previous model version, if needed. - All change decisions are made by human. Flows Here we show the sequential diagrams of the training pipeline: ... Apache Airflow pipelines TODO detail of the Airflow Data Prep workers These are data prep for training pipeline. - Predicted result capture - CFR data capture - Raw input capture -","title":"Model Training Pipeline"},{"location":"lab-env/model-train/#model-training-pipeline","text":"TODO","title":"Model Training Pipeline"},{"location":"lab-env/model-train/#c3-component-diagram","text":"","title":"C3 \u2013 Component Diagram"},{"location":"lab-env/model-train/#conceptual-flow","text":"Below is a conceptual flow of the model training pipeline. ... Design principles: - For the current features used, the model will be shadowing regulary trained. To make sure, we always have a better model standing by. - The model retention period or number can be configurable, to make sure, we can recover from a previous model version, if needed. - All change decisions are made by human.","title":"Conceptual flow"},{"location":"lab-env/model-train/#flows","text":"Here we show the sequential diagrams of the training pipeline: ...","title":"Flows"},{"location":"lab-env/model-train/#apache-airflow-pipelines","text":"TODO detail of the Airflow","title":"Apache Airflow pipelines"},{"location":"lab-env/model-train/#data-prep-workers","text":"These are data prep for training pipeline. - Predicted result capture - CFR data capture - Raw input capture -","title":"Data Prep workers"},{"location":"lab-env/txn-risk-score/","text":"Transaction risk score service Key components There are 3 major components for the transaction risk score. * API services 1. On-demand API service (API for client) 1. Core Prediction service (internal API) * Kafka consumer (worker) 1. Prediction worker (Kafka consumer) Service specification \u2139\ufe0f Both API services are gRPC API servers. Core prediction service (Python API) Python based model serving via API Receive only non-PII inputs This core service is served for the other 2 modules: Kafka consumer (worker) and API server. The inferred result will be output to 3 destinations: caching in Redis, producing to Kafka, and return to the requester. The input features and their predicted result, produced to Kafka, are kept in SeaweedFS, for model evaluation pipeline. Here is its flow: sequenceDiagram participant Req as Score Requestor participant CPS as Core Prediction Service participant R as Redis participant K as Kafka participant SC as Predicted Score Capture participant FS as SeaweedFS Req->>CPS: request with features payload activate CPS activate Req CPS->>CPS: infer CPS->>R: cache the score Note over R: promptguard:transactionscore:<correlation_id> CPS->>K: publish features and score to keep Note over K: promptguard.transactionscore.inferred CPS-->>Req: return predicted score deactivate CPS deactivate Req SC->>K: consume features and predicted score activate SC activate K K-->>SC: features and score deactivate K SC->>FS: save features and score <br> for model evaluation pipeline deactivate SC To evaluate prediction performance later, we need a predicted transaction ID that can be matched with CFR. It is called correlation_id . Here is how we construct the correlation_id (python example): def construct_correlation_id( sender_proxy_id: str, receiver_proxy_id: str, timestamp: str, amount: str, sender_fi_code: Optional[str] = None, receiver_fi_code: Optional[str] = None, sender_proxy_type: str = 'account', receiver_proxy_type: str = 'account', ) -> str: \"\"\" Constructs a short, unique correlation ID for a transaction. Args: sender_proxy_id: The sender's proxy ID (e.g., account number, phone). receiver_proxy_id: The receiver's proxy ID. timestamp: ISO 8601 formatted timestamp of the transaction. amount: The transaction amount in B.SS format (e.g., 100.50, 27.00). sender_fi_code: The sender's financial institution code. Required if proxy type is 'account'. receiver_fi_code: The receiver's financial institution code. Required if proxy type is 'account'. sender_proxy_type: The type of the sender's proxy. receiver_proxy_type: The type of the receiver's proxy. Returns: A 18-character hexadecimal correlation ID. Raises: ValueError: If an FI code is missing for an 'account' proxy type. \"\"\" if (sender_proxy_type == 'account' and not sender_fi_code) or \\ (receiver_proxy_type == 'account' and not receiver_fi_code): raise ValueError(\"Financial institution code is required for 'account' proxy types.\") if sender_proxy_type == 'account': sender_account_number = sender_proxy_id else: sender_fi_code, sender_account_number = proxy_resolve(sender_proxy_id, sender_proxy_type) if receiver_proxy_type == 'account': receiver_account_number = receiver_proxy_id else: receiver_fi_code, receiver_account_number = proxy_resolve(receiver_proxy_id, receiver_proxy_type) # Securely hash the account numbers with the secret sender_account_hash = hash_with_secret(sender_account_number, HASHING_SECRET) receiver_account_hash = hash_with_secret(receiver_account_number, HASHING_SECRET) combined_string = \"|\".join([ str(sender_fi_code), sender_account_hash, str(receiver_fi_code), receiver_account_hash, str(timestamp), str(amount) ]) hasher = hashlib.sha256(combined_string.encode('utf-8')) # Take the first 18 characters from the hexadecimal representation of the hash. correlation_id = hasher.hexdigest()[:18] return correlation_id The 18-characters length is to avoid id collision for 6000 to 8000 million transactions which are kept for 3 months. Kafka consumer (Go worker) This worker consumes non-PII PromptPay messages from Kafka, prepares relevant features, then feeds to the Core prediction service sequenceDiagram participant LE as Log extractor (Blend / Loki) participant KK as Kafka Topic participant TW as Txn score prediction worker participant R as Redis participant CPS as Core Prediction Service LE->>KK: publish anonym txn note over KK: promptguard.transactionscore.from-pp TW->>KK: consume txn activate TW KK-->>TW: txn data TW->>R: get account feature activate R R-->>TW: features deactivate R TW->>CPS: request for a score predict activate CPS CPS->>CPS: infer CPS->>R: cache the score CPS->>KK: publish score to save note over KK: promptguard.transactionscore.inferred CPS-->>TW: return predicted score deactivate CPS deactivate TW The worker is scaled to handle amount of work load (messages) from Kafka topic. The feature materials will be got from Redis. They are prepared by real-time transaction data preparation module beforehand. The worker will compose required features then feed to Core prediction service On-demand transaction score service (Go API) This module will be called by API gateway to serve transaction score on-demand. sequenceDiagram participant C as Client participant A as Apigee participant TS as Txn score svc participant R as Redis participant CPS as Core Prediction Service participant K as Kafka C->>A: Txn score req activate A activate C A->>TS: pass auth / required fields activate TS TS->>TS: biz logic validate TS->>R: Qry cached score activate R alt cache hit R-->>TS: return score deactivate R else cache miss activate R R-->>TS: key not found deactivate R TS->>CPS: request for a fresh score predict activate CPS CPS->>CPS: infer CPS->>R: cache the score CPS->>K: publish score to save Note over K: promptguard.transactionscore.inferred CPS-->>TS: return predicted score deactivate CPS end TS-->>A: return txn score deactivate TS A-->>C: return txn score deactivate A deactivate C In case that /txn-score/svc/qry cannot get any predicted score in Redis, it needs to anonymize PII data before calling the /txn-score/predict/infer . The salt (password to set in the hashing algorithm) must be configurable via k8s secret. This headless API service must be fast and lean. Golang is expected to be used. API specification Below is the overview of the API spec. HTTP Header: Header name Description X-API-Version v1 API Services: Module API Version Method Description Transaction score service /txn-score/svc/health v1 GET Service health check /txn-score/svc/qry v1 POST For client to query a predicted score Core transaction score prediction /txn-score/predict/health v1 GET Service health check /txn-score/predict/infer v1 POST For internal score inference. An on-demand based score calculation used if there is no cached score available For more detail of the API see OpenAPI spec: promptguard-txn-score-api-spec-v0.2.yaml Interface Protobuf: promptguard-txn-score-v0.1.proto Redis schema Key naming convention: see: Key naming convention Example: promptguard:transactionscore:95248989ec91f6d043 Value Data schema: { \"correlation_id\": \"95248989ec91f6d043\", \"pp_txn_id\": \"202408132300010042024081323000180\", \"risk_score\": 23, \"timestamp\": \"2024-05-13T09:45:33+07:00\" } Field Occurence Data type Length / Range Description correlation_id [1..1] string Max: 20 PromptGuard correlation ID in 18 lower-case hex characters pp_txn_id [1..1] string Max: 50 PromptPay transaction ID. Detail as below risk_score [1..1] integer 0 to 100 Infered transaction risk score. Higher score means more risk. timestamp [1..1] string 25 Format: ISO-8601 datetime Pattern: YYYY-MM-DDThh:mm:ssZ PromptPay transaction ID is from account lookup message. It is different for ISO8583 and ISO20022. For ISO8583, it is a concat string of: TSTAMP_LOCAL + INST_ID_RECON_ACQ + RETRIEVAL_REF_NO + TRAN_CLASS . For ISO20022, it is RETRIEVAL_REF_NO . Kafka topic and message schema Naming: see: Topic naming convention Here are the Kafka topics used in this module: promptguard.transactionscore.from-pp This topic keeps anonymized transactions streaming from PromptPay in real-time. promptguard.transactionscore.infered This topic keeps inferred score from Core Prediction Service. There will be a scheduled consumer save this score to SeaweedFS for the evaluation pipeline. PromptPay message schema This is expected message schema of promptguard.transactionscore.from-pp : Field Occurence Data type Length Description Example ACCT_1_ID [1..1] string 40 Sender account number d7KW4uuNmV+Li9hbecdLz8MYEQpfcdrHgj5UnNk3DsU= ACCT_1_NAME [1..1] string 150 Sender account name mXFKL6pisfDLOKIGBMRVHMz84kudzSS7ETulJIqbLEY= ACCT_2_ID [1..1] string 40 Receiver account number 6Z1RaS0p9GcjFRV0nLIyvDmNhkvTqGqQwcaNw/rJQ4o= ACCT_2_NAME [1..1] string 150 Receiver account name udWoJ/3DUAkc8zZClxFvGU6H/3IqPZDwOccrmE7WeiLBe1hQS6ef9rgAu9L1THKG/G/+0ZOJfjppLiCwGFw7Aw== ACT_CODE [1..1] string 3 Status code: accept = 000 000 AMT_RECON_NET [1..1] string 18 unit: satang +000000000000002400. BILL_REF1 [1..1] string 20 BILL_REF2 [1..1] string 20 BILL_REF3 [1..1] string 20 FROM_ISO [1..1] string Max: 5 Original PP message schema. 8583 or 20022 20022 INST_ID_RECON_ACQ [1..1] string 3 sender BANK_CODE 004 INST_ID_RECON_ISS [1..1] string 3 receiver BANK_CODE 014 RECV_PROXY_ID [1..1] string 128 010753600031508 RECV_PROXY_TYPE [1..1] string 12 BILLERID RECV_TYPE [1..1] string 1 H RETRIEVAL_REF_NO [1..1] string 12 ~RRN - internal transaction id (same ID for all 4-leg of pp transactions) 155959 SEND_TYPE [1..1] string 1 H TERM_CLASS [1..1] string 2 User access channel see the mapping table below. 80 TRAN_CLASS [1..1] string 3 transcation: CTF (credit transfer) BPA TSTAMP_LOCAL [0..1] string 14 Time at user application in GMT+7. Optional field, avaliable only in ISO8583 20240813230001 TSTAMP_TRANS [1..1] string 14 to 16 Tranmission date time to ITMX (Original message in GMT). Should be the same as TSTAMP_LOCAL . Legth 16 for 8583 and 14 for 20022. 2024081323000100 20240401000049 TERM_CLASS Channel 10 IVR 20 Kiosk 30 ATM 40 EDC/POS 50 Counter 60 Internet 70 CDM 80 Mobile Predicted score and its features schema This is expected message schema of promptguard.transactionscore.infered : { \"correlation_id\": \"95248989ec91f6d043\", \"pp_txn_id\": \"202408132300010042024081323000180\", \"risk_score\": 23, \"timestamp\": \"2024-05-13T09:45:33+07:00\", \"features\": { <FEATURE_DATA> } } The Predicted Score Capture worker will consume this data and save to SeaweedFS for the evaluation pipeline. Prediction Features There are 2 kinds of prediction features used, near real-time and daily features. For more detail see: Prediction Feature Updater","title":"Transaction Risk Score"},{"location":"lab-env/txn-risk-score/#transaction-risk-score-service","text":"","title":"Transaction risk score service"},{"location":"lab-env/txn-risk-score/#key-components","text":"There are 3 major components for the transaction risk score. * API services 1. On-demand API service (API for client) 1. Core Prediction service (internal API) * Kafka consumer (worker) 1. Prediction worker (Kafka consumer)","title":"Key components"},{"location":"lab-env/txn-risk-score/#service-specification","text":"\u2139\ufe0f Both API services are gRPC API servers.","title":"Service specification"},{"location":"lab-env/txn-risk-score/#core-prediction-service-python-api","text":"Python based model serving via API Receive only non-PII inputs This core service is served for the other 2 modules: Kafka consumer (worker) and API server. The inferred result will be output to 3 destinations: caching in Redis, producing to Kafka, and return to the requester. The input features and their predicted result, produced to Kafka, are kept in SeaweedFS, for model evaluation pipeline. Here is its flow: sequenceDiagram participant Req as Score Requestor participant CPS as Core Prediction Service participant R as Redis participant K as Kafka participant SC as Predicted Score Capture participant FS as SeaweedFS Req->>CPS: request with features payload activate CPS activate Req CPS->>CPS: infer CPS->>R: cache the score Note over R: promptguard:transactionscore:<correlation_id> CPS->>K: publish features and score to keep Note over K: promptguard.transactionscore.inferred CPS-->>Req: return predicted score deactivate CPS deactivate Req SC->>K: consume features and predicted score activate SC activate K K-->>SC: features and score deactivate K SC->>FS: save features and score <br> for model evaluation pipeline deactivate SC To evaluate prediction performance later, we need a predicted transaction ID that can be matched with CFR. It is called correlation_id . Here is how we construct the correlation_id (python example): def construct_correlation_id( sender_proxy_id: str, receiver_proxy_id: str, timestamp: str, amount: str, sender_fi_code: Optional[str] = None, receiver_fi_code: Optional[str] = None, sender_proxy_type: str = 'account', receiver_proxy_type: str = 'account', ) -> str: \"\"\" Constructs a short, unique correlation ID for a transaction. Args: sender_proxy_id: The sender's proxy ID (e.g., account number, phone). receiver_proxy_id: The receiver's proxy ID. timestamp: ISO 8601 formatted timestamp of the transaction. amount: The transaction amount in B.SS format (e.g., 100.50, 27.00). sender_fi_code: The sender's financial institution code. Required if proxy type is 'account'. receiver_fi_code: The receiver's financial institution code. Required if proxy type is 'account'. sender_proxy_type: The type of the sender's proxy. receiver_proxy_type: The type of the receiver's proxy. Returns: A 18-character hexadecimal correlation ID. Raises: ValueError: If an FI code is missing for an 'account' proxy type. \"\"\" if (sender_proxy_type == 'account' and not sender_fi_code) or \\ (receiver_proxy_type == 'account' and not receiver_fi_code): raise ValueError(\"Financial institution code is required for 'account' proxy types.\") if sender_proxy_type == 'account': sender_account_number = sender_proxy_id else: sender_fi_code, sender_account_number = proxy_resolve(sender_proxy_id, sender_proxy_type) if receiver_proxy_type == 'account': receiver_account_number = receiver_proxy_id else: receiver_fi_code, receiver_account_number = proxy_resolve(receiver_proxy_id, receiver_proxy_type) # Securely hash the account numbers with the secret sender_account_hash = hash_with_secret(sender_account_number, HASHING_SECRET) receiver_account_hash = hash_with_secret(receiver_account_number, HASHING_SECRET) combined_string = \"|\".join([ str(sender_fi_code), sender_account_hash, str(receiver_fi_code), receiver_account_hash, str(timestamp), str(amount) ]) hasher = hashlib.sha256(combined_string.encode('utf-8')) # Take the first 18 characters from the hexadecimal representation of the hash. correlation_id = hasher.hexdigest()[:18] return correlation_id The 18-characters length is to avoid id collision for 6000 to 8000 million transactions which are kept for 3 months.","title":"Core prediction service (Python API)"},{"location":"lab-env/txn-risk-score/#kafka-consumer-go-worker","text":"This worker consumes non-PII PromptPay messages from Kafka, prepares relevant features, then feeds to the Core prediction service sequenceDiagram participant LE as Log extractor (Blend / Loki) participant KK as Kafka Topic participant TW as Txn score prediction worker participant R as Redis participant CPS as Core Prediction Service LE->>KK: publish anonym txn note over KK: promptguard.transactionscore.from-pp TW->>KK: consume txn activate TW KK-->>TW: txn data TW->>R: get account feature activate R R-->>TW: features deactivate R TW->>CPS: request for a score predict activate CPS CPS->>CPS: infer CPS->>R: cache the score CPS->>KK: publish score to save note over KK: promptguard.transactionscore.inferred CPS-->>TW: return predicted score deactivate CPS deactivate TW The worker is scaled to handle amount of work load (messages) from Kafka topic. The feature materials will be got from Redis. They are prepared by real-time transaction data preparation module beforehand. The worker will compose required features then feed to Core prediction service","title":"Kafka consumer (Go worker)"},{"location":"lab-env/txn-risk-score/#on-demand-transaction-score-service-go-api","text":"This module will be called by API gateway to serve transaction score on-demand. sequenceDiagram participant C as Client participant A as Apigee participant TS as Txn score svc participant R as Redis participant CPS as Core Prediction Service participant K as Kafka C->>A: Txn score req activate A activate C A->>TS: pass auth / required fields activate TS TS->>TS: biz logic validate TS->>R: Qry cached score activate R alt cache hit R-->>TS: return score deactivate R else cache miss activate R R-->>TS: key not found deactivate R TS->>CPS: request for a fresh score predict activate CPS CPS->>CPS: infer CPS->>R: cache the score CPS->>K: publish score to save Note over K: promptguard.transactionscore.inferred CPS-->>TS: return predicted score deactivate CPS end TS-->>A: return txn score deactivate TS A-->>C: return txn score deactivate A deactivate C In case that /txn-score/svc/qry cannot get any predicted score in Redis, it needs to anonymize PII data before calling the /txn-score/predict/infer . The salt (password to set in the hashing algorithm) must be configurable via k8s secret. This headless API service must be fast and lean. Golang is expected to be used.","title":"On-demand transaction score service (Go API)"},{"location":"lab-env/txn-risk-score/#api-specification","text":"Below is the overview of the API spec. HTTP Header: Header name Description X-API-Version v1 API Services: Module API Version Method Description Transaction score service /txn-score/svc/health v1 GET Service health check /txn-score/svc/qry v1 POST For client to query a predicted score Core transaction score prediction /txn-score/predict/health v1 GET Service health check /txn-score/predict/infer v1 POST For internal score inference. An on-demand based score calculation used if there is no cached score available For more detail of the API see OpenAPI spec: promptguard-txn-score-api-spec-v0.2.yaml Interface Protobuf: promptguard-txn-score-v0.1.proto","title":"API specification"},{"location":"lab-env/txn-risk-score/#redis-schema","text":"Key naming convention: see: Key naming convention Example: promptguard:transactionscore:95248989ec91f6d043 Value Data schema: { \"correlation_id\": \"95248989ec91f6d043\", \"pp_txn_id\": \"202408132300010042024081323000180\", \"risk_score\": 23, \"timestamp\": \"2024-05-13T09:45:33+07:00\" } Field Occurence Data type Length / Range Description correlation_id [1..1] string Max: 20 PromptGuard correlation ID in 18 lower-case hex characters pp_txn_id [1..1] string Max: 50 PromptPay transaction ID. Detail as below risk_score [1..1] integer 0 to 100 Infered transaction risk score. Higher score means more risk. timestamp [1..1] string 25 Format: ISO-8601 datetime Pattern: YYYY-MM-DDThh:mm:ssZ PromptPay transaction ID is from account lookup message. It is different for ISO8583 and ISO20022. For ISO8583, it is a concat string of: TSTAMP_LOCAL + INST_ID_RECON_ACQ + RETRIEVAL_REF_NO + TRAN_CLASS . For ISO20022, it is RETRIEVAL_REF_NO .","title":"Redis schema"},{"location":"lab-env/txn-risk-score/#kafka-topic-and-message-schema","text":"Naming: see: Topic naming convention Here are the Kafka topics used in this module: promptguard.transactionscore.from-pp This topic keeps anonymized transactions streaming from PromptPay in real-time. promptguard.transactionscore.infered This topic keeps inferred score from Core Prediction Service. There will be a scheduled consumer save this score to SeaweedFS for the evaluation pipeline.","title":"Kafka topic and message schema"},{"location":"lab-env/txn-risk-score/#promptpay-message-schema","text":"This is expected message schema of promptguard.transactionscore.from-pp : Field Occurence Data type Length Description Example ACCT_1_ID [1..1] string 40 Sender account number d7KW4uuNmV+Li9hbecdLz8MYEQpfcdrHgj5UnNk3DsU= ACCT_1_NAME [1..1] string 150 Sender account name mXFKL6pisfDLOKIGBMRVHMz84kudzSS7ETulJIqbLEY= ACCT_2_ID [1..1] string 40 Receiver account number 6Z1RaS0p9GcjFRV0nLIyvDmNhkvTqGqQwcaNw/rJQ4o= ACCT_2_NAME [1..1] string 150 Receiver account name udWoJ/3DUAkc8zZClxFvGU6H/3IqPZDwOccrmE7WeiLBe1hQS6ef9rgAu9L1THKG/G/+0ZOJfjppLiCwGFw7Aw== ACT_CODE [1..1] string 3 Status code: accept = 000 000 AMT_RECON_NET [1..1] string 18 unit: satang +000000000000002400. BILL_REF1 [1..1] string 20 BILL_REF2 [1..1] string 20 BILL_REF3 [1..1] string 20 FROM_ISO [1..1] string Max: 5 Original PP message schema. 8583 or 20022 20022 INST_ID_RECON_ACQ [1..1] string 3 sender BANK_CODE 004 INST_ID_RECON_ISS [1..1] string 3 receiver BANK_CODE 014 RECV_PROXY_ID [1..1] string 128 010753600031508 RECV_PROXY_TYPE [1..1] string 12 BILLERID RECV_TYPE [1..1] string 1 H RETRIEVAL_REF_NO [1..1] string 12 ~RRN - internal transaction id (same ID for all 4-leg of pp transactions) 155959 SEND_TYPE [1..1] string 1 H TERM_CLASS [1..1] string 2 User access channel see the mapping table below. 80 TRAN_CLASS [1..1] string 3 transcation: CTF (credit transfer) BPA TSTAMP_LOCAL [0..1] string 14 Time at user application in GMT+7. Optional field, avaliable only in ISO8583 20240813230001 TSTAMP_TRANS [1..1] string 14 to 16 Tranmission date time to ITMX (Original message in GMT). Should be the same as TSTAMP_LOCAL . Legth 16 for 8583 and 14 for 20022. 2024081323000100 20240401000049 TERM_CLASS Channel 10 IVR 20 Kiosk 30 ATM 40 EDC/POS 50 Counter 60 Internet 70 CDM 80 Mobile","title":"PromptPay message schema"},{"location":"lab-env/txn-risk-score/#predicted-score-and-its-features-schema","text":"This is expected message schema of promptguard.transactionscore.infered : { \"correlation_id\": \"95248989ec91f6d043\", \"pp_txn_id\": \"202408132300010042024081323000180\", \"risk_score\": 23, \"timestamp\": \"2024-05-13T09:45:33+07:00\", \"features\": { <FEATURE_DATA> } } The Predicted Score Capture worker will consume this data and save to SeaweedFS for the evaluation pipeline.","title":"Predicted score and its features schema"},{"location":"lab-env/txn-risk-score/#prediction-features","text":"There are 2 kinds of prediction features used, near real-time and daily features. For more detail see: Prediction Feature Updater","title":"Prediction Features"},{"location":"prod-env/","text":"TODO Production Environment","title":"Prod Environment"},{"location":"prod-env/#todo-production-environment","text":"","title":"TODO Production Environment"}]}